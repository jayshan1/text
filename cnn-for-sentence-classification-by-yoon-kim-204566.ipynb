{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10025,"databundleVersionId":32092,"sourceType":"competition"},{"sourceId":3176,"sourceType":"datasetVersion","datasetId":1835}],"dockerImageVersionId":12836,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Convolutional Neural Networks for Sentence Classification by Yoon Kim in Keras\n\n[Kim's paper](http://www.aclweb.org/anthology/D14-1181) was published in 2014 and showed that not only are CNNs great for images but also text. With an extreamly simple architecture Kim outperformed all other models in 4 out of 7 benchmarks at the time. In my first attempt to reproduce a paper on Kaggle, I'm going to try and recreate what he did with Keras.\n\n## Architecture\n\n![Architecture diagram](https://imgur.com/Dsu7Q61.png)\n\n*- architecture diagram taken from Kim's paper*\n\nThe architecture Kim used goes like this:\n\nFirstly, sentences are represented as vectors of words and those words are converted into (300D) vectors giving us a 2D representation for each sentence. Kim uses a few different approaches for creating these word vectors:\n\n- **CNN-rand**: where embeddings are randomly assigned to each word\n- **CNN-static**: word2vec is used to provide word embeddings. Unknown words are randomly initialised. These embeddings are kept fixed.\n- **CNN-non-static**: as above, but the vectors are fine-tuned (ie they can be changed) during training\n\nKim also tried another approach which I won't attempt to do here.. because quite frankly I don't know how. I might attempt this in the future.\n\n- **CNN-multichannel**: Here Kim uses two sets of word vectors. My understanding is he applies both to the sentence (resulting in two, sentence_length x 300D matricies) and performs convolutions on both (see below). During training he fine tunes one embedding but keeps the other one fixed. He states the hope of this was that it would act as a form of regularization but the results were mixed. I'm not sure if this approach was investigated and improved upon in other papers or not. Hopefully it was, it sounds like the kind of thing that might work.\n\nHe then performs convolutions on these 2D representations, but he does it in a way you don't see very often. Convolutions of different window sizes (3, 4 and 5) are performed on the representations directly and then max pooled. This is a little like an inception module in the Goog LeNet, but not something I've really seen anywhere else.\n\nFinally after a layout of dropout, an output layer is directly used to do the predictions.\n\nAnd that's it! With one conv layer and some max pooling we can get fantastic results!\n\n\n### Why this paper is interesting\n\nWe all know LSTMs are the king of sentence classification... but are they really?\n\nRecently I tried to classify a million sentences where I work with a (admittedly complicated) LSTM. It was estimated it would take 130 hours to run on the machine available. With something inspired by this model it takes around an hour and we achieved compariable accuracy (these are short sentences which is believe why that works).\n\nIt's also interesting to see totally different approaches to solving this problem.\n\n**Update** while working on another problem I found myself doing something Kim does in this paper which you don't often see:\n\n> When randomly initializing words not in word2vec, we obtained slight improvements by sampling each dimension from `U[−a, a]` where a was chosen such that the randomly initialized vectors have the same variance as the pre-trained ones. It would be interesting to see if employing more sophisticated methods to mirror the distribution of pre-trained vectors in the initialization process gives further improvements.\n\nThis is really interesting and something I would like to explore in another kernel.\n\n\n### Places this differs from Kim's paper\n\nSadly I couldn't find the same data Kim used, but I've managed to find something very similar sounding to the SST1 data (from which I can infer the SST2 data) where it looks like we can achieve simiar results.\n\nKim uses a 300D w2v model trained using cbow. I couldn't find this model, but used a 200D model trained using GloVe. The major impact of this is we use an embedding dimension of 200 here.\n\nI also swapped out the optimizer Kim used (AdaDelta) for Adam - AdaDelta took forever to run but it looks like you get the same results as Adam. If you're happy to sit there for around 100 epochs you can try it out.\n\n\n# Data prep\n\nSo let's get started. Before we do anything else we need to get some data and prep it","metadata":{"_uuid":"3a184cf39b80e4f3ded8afb4d4a59a905915f1b4"}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":false,"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets import some stuff\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.initializers import Constant\nfrom keras.models import Model\nfrom keras.layers import *\nfrom keras.utils.np_utils import to_categorical\nimport re\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"_uuid":"08c3425b67173c7162a586c06cbb279f29f946a9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/train.tsv', delimiter='\\t')\ndf = df[['Phrase', 'Sentiment']]\n\npd.set_option('display.max_colwidth', -1)\ndf.head(3)","metadata":{"_uuid":"0e0fe2cee592ee4313e67b342fffdd9bb851578d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, you can see that the `Phrase` data which we're going to train our model against hasn't been cleaned so we need to do it.\n\nI am just going to do something very simple, which is to turn url's into `url`, remove anything that's not alphanumeric or a space. Then lowercase what's left.","metadata":{"_uuid":"0df307c3d8d0ea1bd773064e9eb1ba30ced73848"}},{"cell_type":"code","source":"def clean_str(in_str):\n    in_str = str(in_str)\n    # replace urls with 'url'\n    in_str = re.sub(r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})\", \"url\", in_str)\n    in_str = re.sub(r'([^\\s\\w]|_)+', '', in_str)\n    return in_str.strip().lower()\n\n\ndf['text'] = df['Phrase'].apply(clean_str)","metadata":{"_uuid":"1560d4c17410b19f0d420f4afb074e0721b15e56","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our data is classified into 5 different classes, very negative, slightly negative, neutral, slightly positive and very positive.\n\nSadly our dataset isn't balanced, so we need to do that ourselves","metadata":{"_uuid":"ec14d0e2bb4fcfcee507442015c643ae3df76e46","trusted":true}},{"cell_type":"code","source":"df.Sentiment.value_counts()","metadata":{"_uuid":"0a42e34a57fa9e55a4a613ce60d7b5f6314a6faf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_0 = df[df['Sentiment'] == 0].sample(frac=1)\ndf_1 = df[df['Sentiment'] == 1].sample(frac=1)\ndf_2 = df[df['Sentiment'] == 2].sample(frac=1)\ndf_3 = df[df['Sentiment'] == 3].sample(frac=1)\ndf_4 = df[df['Sentiment'] == 4].sample(frac=1)\n\n# we want a balanced set for training against - there are 7072 `0` examples\nsample_size = 7072\n\ndata = pd.concat([df_0.head(sample_size), df_1.head(sample_size), df_2.head(sample_size), df_3.head(sample_size), df_4.head(sample_size)]).sample(frac=1)","metadata":{"_uuid":"ecbc5ac7b4d7195cf65050b557ca0d9a383032b4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we need to tokenize our sentences into vectors. Before we do that we need to pick a vector size.","metadata":{"_uuid":"25e8946dae515f87a21959d4387566de945d2154"}},{"cell_type":"code","source":"data['l'] = data['Phrase'].apply(lambda x: len(str(x).split(' ')))\nprint(\"mean length of sentence: \" + str(data.l.mean()))\nprint(\"max length of sentence: \" + str(data.l.max()))\nprint(\"std dev length of sentence: \" + str(data.l.std()))","metadata":{"_uuid":"ec44d5d08736cf56b40718228427a50f7355f1f1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# these sentences aren't that long so we may as well use the whole string\nsequence_length = 52","metadata":{"_uuid":"0c546ced21a6fc3e8ffb9747e192e7da364911a2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_features = 20000 # this is the number of words we care about\n\ntokenizer = Tokenizer(num_words=max_features, split=' ', oov_token='<unw>')\ntokenizer.fit_on_texts(data['Phrase'].values)\n\n# this takes our sentences and replaces each word with an integer\nX = tokenizer.texts_to_sequences(data['Phrase'].values)\n\n# we then pad the sequences so they're all the same length (sequence_length)\nX = pad_sequences(X, sequence_length)\n\ny = pd.get_dummies(data['Sentiment']).values\n\n# where there isn't a test set, Kim keeps back 10% of the data for testing, I'm going to do the same since we have an ok amount to play with\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n\nprint(\"test set size \" + str(len(X_test)))","metadata":{"_uuid":"e7d26ca92f2f3bf193df765fa1621d85bdf22297","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 1: Random embeddings\n\nLets build our model. In general I'm going to just use the same hyperparameters as Kim does (see section 3.1 of his paper) apart from the embedding dimension\n\nKeras has an Embedding layer we can use here. If you don't specify a custom way to embed text (something we will do later with w2v) Keras will do it randomly with a normal (Gaussian) distribution for you","metadata":{"_uuid":"461ca35a112a721e3c739c3aa82599db6e36c499"}},{"cell_type":"code","source":"embedding_dim = 200 # Kim uses 300 here\nnum_filters = 100\n\ninputs = Input(shape=(sequence_length,), dtype='int32')\n\n# use a random embedding for the text\nembedding_layer = Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n\nreshape = Reshape((sequence_length, embedding_dim, 1))(embedding_layer)\n\n# Note the relu activation which Kim specifically mentions\n# He also uses an l2 constraint of 3\n# Also, note that the convolution window acts on the whole 200 dimensions - that's important\nconv_0 = Conv2D(num_filters, kernel_size=(3, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\nconv_1 = Conv2D(num_filters, kernel_size=(4, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\nconv_2 = Conv2D(num_filters, kernel_size=(5, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\n\n# perform max pooling on each of the convoluations\nmaxpool_0 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0)\nmaxpool_1 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1)\nmaxpool_2 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2)\n\n# concat and flatten\nconcatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\nflatten = Flatten()(concatenated_tensor)\n\n# do dropout and predict\ndropout = Dropout(0.5)(flatten)\noutput = Dense(units=5, activation='softmax')(dropout)","metadata":{"_uuid":"6efb6f8c0974ae0a5b12c4374ac9b4b9b0f217ac","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Kim uses Adadelta as his optimizer. After some experimentation it's clear you get the same results with adam, it just happens to get there MUCH slower with adadelta (which makes sense). For everyone's sanity I'm going to use adam here - if you'd like to follow Kim's paper closer than me then all you have to do is swap `adam` for `adadelta` in the next cell and increase the number of epochs to around 100","metadata":{"_uuid":"747582d6ec9c133dd719ce8ed9017b8baa870c56"}},{"cell_type":"code","source":"model = Model(inputs=inputs, outputs=output)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","metadata":{"_uuid":"264a64231f955b32197618dc83c26a907635bc1c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32 # Kim uses 50 here, I have a slightly smaller sample size than num\nhistory = model.fit(X_train, y_train, epochs=30, batch_size=batch_size, verbose=1, validation_split=0.1, shuffle=True)","metadata":{"_uuid":"cb108fefc0af9f24fa738f7d67713848cb626b31","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's plot the training","metadata":{"_uuid":"51336f27d423d15777d68a3cae509adab84aa3a9"}},{"cell_type":"code","source":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","metadata":{"_uuid":"cd8c6247c101d7e343c7b6f21239ea558cd16429","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"as you can see this is a bit noisey, but it doesn't overfit too badly or do crazy things, so I'm happy!\n\nSo how good is this model? Lets test it with our test set","metadata":{"_uuid":"054cd8161230abe3c31ceb0eb53b369b11562d91"}},{"cell_type":"code","source":"y_hat = model.predict(X_test)","metadata":{"_uuid":"9d050fea58cf2e351274435b3c9faa9d9ae99854","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat)))","metadata":{"_uuid":"d08a240de0b06b5a3042be125a1895c593135a4e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat)))","metadata":{"_uuid":"adcaa3b5972bda1b93cc270c6f50de66eb8708c9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is suprisingly good - a random model would achieve a score of ~0.2 (there are 5 classes)","metadata":{"_uuid":"ae5504b1efeadc3ccf247e88a5b6468540bf05bd"}},{"cell_type":"markdown","source":"# Model 2: Static word2vec\n\nNow rather than randomly assign vectors we're going use w2v embeddings. This took me quite a long time to get right, so I'll walk through it line by line\n\nFirstly, we need to load the model. As mentioned I couldn't find the model Kim used, but Kaggle does have a great 200D model trained using GloVe.","metadata":{"_uuid":"c34ecf0298d9a96625397833231ff872723e96f3"}},{"cell_type":"code","source":"embeddings_index = {}\nf = open(os.path.join('../input/glove-global-vectors-for-word-representation', 'glove.6B.200d.txt'))\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","metadata":{"_uuid":"ce8803408785f0222cdb4d424c574135fe0b8d2e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","metadata":{"_uuid":"573d5e1e5b170a77f61ab66990ba4cb6bc8b66a4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now for the fun bit, we run through all the words in our tokenizer and look for them in the w2v model. If they exist we add them to our embedding matrix (ie our embeddings) if they don't then we assign a random vector","metadata":{"_uuid":"a28233126e7ce05bde9cf1c6fdbff62f5bd62d66"}},{"cell_type":"code","source":"num_words = min(max_features, len(word_index)) + 1\nprint(num_words)\n\n# first create a matrix of zeros, this is our embedding matrix\nembedding_matrix = np.zeros((num_words, embedding_dim))\n\n# for each word in out tokenizer lets try to find that work in our w2v model\nfor word, i in word_index.items():\n    if i > max_features:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # we found the word - add that words vector to the matrix\n        embedding_matrix[i] = embedding_vector\n    else:\n        # doesn't exist, assign a random vector\n        embedding_matrix[i] = np.random.randn(embedding_dim)","metadata":{"_uuid":"6ab06dd855b04f70166527630ffdbe72b64d7d0f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then our model looks almost identical to the first model, except with a different embedding layer","metadata":{"_uuid":"1ae26e909f2b524d654aa50b9d8c7127630da00c"}},{"cell_type":"code","source":"inputs_2 = Input(shape=(sequence_length,), dtype='int32')\n\n# note the `trainable=False`, later we will make this layer trainable\nembedding_layer_2 = Embedding(num_words,\n                            embedding_dim,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=sequence_length,\n                            trainable=False)(inputs_2)\n\nreshape_2 = Reshape((sequence_length, embedding_dim, 1))(embedding_layer_2)\n\nconv_0_2 = Conv2D(num_filters, kernel_size=(3, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_2)\nconv_1_2 = Conv2D(num_filters, kernel_size=(4, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_2)\nconv_2_2 = Conv2D(num_filters, kernel_size=(5, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_2)\n\nmaxpool_0_2 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0_2)\nmaxpool_1_2 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1_2)\nmaxpool_2_2 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2_2)\n\nconcatenated_tensor_2 = Concatenate(axis=1)([maxpool_0_2, maxpool_1_2, maxpool_2_2])\nflatten_2 = Flatten()(concatenated_tensor_2)\n\ndropout_2 = Dropout(0.5)(flatten_2)\noutput_2 = Dense(units=5, activation='softmax')(dropout_2)","metadata":{"_uuid":"e63faa267307e89d72ecc591de05b6ae036d3c50","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_2 = Model(inputs=inputs_2, outputs=output_2)\nmodel_2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model_2.summary())","metadata":{"_uuid":"33cd071c9b72e11641ef8c53fc016ef2ddc42aa4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\nhistory_2 = model_2.fit(X_train, y_train, epochs=30, batch_size=batch_size, verbose=1, validation_split=0.2)","metadata":{"_uuid":"0ac83b5caf0d64aac8aea8debac7d731d60cc4ca","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history_2.history['acc'])\nplt.plot(history_2.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\nplt.plot(history_2.history['loss'])\nplt.plot(history_2.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","metadata":{"_uuid":"30e5f8be7138bfb5646353d4456bdeed06a07d0e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_hat_2 = model_2.predict(X_test)","metadata":{"_uuid":"3ae4f039d0b614086acabc2cca68a5ce102bd924","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_2)))","metadata":{"_uuid":"98f136828c411234d9a3175feb9ebf0669739527","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_2)))","metadata":{"_uuid":"607f795a1df687b27d109f5c72d1f8be34e5aa01","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So you can see this model doesn't do that much better (maybe a bit worse depending how this run went) than the random initialization. This agrees with what Kim found.","metadata":{"_uuid":"d5763c587ddb1db0624b2753d31c5e05d8407a20"}},{"cell_type":"markdown","source":"# Model 3: w2v with trainable embeddings\n\nFor this model we're going to try the same model again, but this time make the embeddings trainable. That means if during training the model decides on a better embedding for a word then it'll update it\n\nThis has the added benifit of updating the words which were randomly assigned a vector (because they weren't in the w2v model)","metadata":{"_uuid":"5268ddd4d449ce7540b7ef7269427e921731e3e8"}},{"cell_type":"code","source":"inputs_3 = Input(shape=(sequence_length,), dtype='int32')\nembedding_layer_3 = Embedding(num_words,\n                            embedding_dim,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=sequence_length,\n                            trainable=True)(inputs_3)\n\nreshape_3 = Reshape((sequence_length, embedding_dim, 1))(embedding_layer_3)\n\n# note the relu activation\nconv_0_3 = Conv2D(num_filters, kernel_size=(3, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_3)\nconv_1_3 = Conv2D(num_filters, kernel_size=(4, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_3)\nconv_2_3 = Conv2D(num_filters, kernel_size=(5, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_3)\n\nmaxpool_0_3 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0_3)\nmaxpool_1_3 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1_3)\nmaxpool_2_3 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2_3)\n\nconcatenated_tensor_3 = Concatenate(axis=1)([maxpool_0_3, maxpool_1_3, maxpool_2_3])\nflatten_3 = Flatten()(concatenated_tensor_3)\n\ndropout_3 = Dropout(0.5)(flatten_3)\noutput_3 = Dense(units=5, activation='softmax')(dropout_3)","metadata":{"_uuid":"c36ceefeeaa74f76bae42c5ac4d39691d6caa219","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_3 = Model(inputs=inputs_3, outputs=output_3)\nmodel_3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model_3.summary())","metadata":{"_uuid":"4560d13f8279ab60e5032ae79b3e30430220109e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\nhistory_3 = model_3.fit(X_train, y_train, epochs=30, batch_size=batch_size, verbose=1, validation_split=0.2)","metadata":{"_uuid":"db3b163e78d48fbaff9de0847548aad06b2bc90f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history_3.history['acc'])\nplt.plot(history_3.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\nplt.plot(history_3.history['loss'])\nplt.plot(history_3.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","metadata":{"_uuid":"0180a0bb915f291cf9bbd2f5d3500cd1a5e7cd81","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_hat_3 = model_3.predict(X_test)","metadata":{"_uuid":"581bc847d6b10330d76eacfc22fdafe66f961fb2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_3)))","metadata":{"_uuid":"e54f6dd1fd37ec8a8fdf8ec75f814c4c6e54261c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_3)))","metadata":{"_uuid":"b9762538c6f57badf6062e78a040df99282f6d08","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see this performs much better than that random vector and static w2v models - by 10% which is a massive improvement!","metadata":{"_uuid":"c32a65a441053d5087f691562d51700d6e1b6266"}},{"cell_type":"markdown","source":"# Model 4: Again, but with binary classification (similar to SST2)\n\nKim tried his models against a varity of datasets. Two of these datasets are closely related to one another (and similar to our dataset).\n\nSST1 is a semantic dataset release by Stanford. It has 5 categories of sentiment (like our data). SST2 is the same dataset, but with very negative and slightly negative merged into one class and slightly positive and positive also merged (neutral is dropped). This gives us a binary classification we can replicate with our data","metadata":{"_uuid":"e1ce30795b88c489b15e97624d1b82afda8322bb"}},{"cell_type":"code","source":"# SST2 is the same as SST1, but with neutrals removed and binary labels\nsst2_data = pd.concat([df_0.head(sample_size), df_1.head(sample_size), df_3.head(sample_size), df_4.head(sample_size)]).sample(frac=1)\n\ndef merge_sentiments(x):\n    if x == 0 or x == 1:\n        return 0\n    else:\n        return 1\n\nsst2_data['Sentiment'] = sst2_data['Sentiment'].apply(merge_sentiments)\n\nsst2_tokenizer = Tokenizer(num_words=max_features, split=' ', oov_token='<unw>')\nsst2_tokenizer.fit_on_texts(sst2_data['Phrase'].values)\n\nsst2_X = sst2_tokenizer.texts_to_sequences(sst2_data['Phrase'].values)\nsst2_X = pad_sequences(sst2_X, sequence_length)\n\nsst2_y = sst2_data['Sentiment'].values\n\nsst2_X_train, sst2_X_test, sst2_y_train, sst2_y_test = train_test_split(sst2_X, sst2_y, test_size=0.1)","metadata":{"_uuid":"cac9094cfd84393e0426b5955fe9f3ba52b5aef7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now lets try to train our model again with this dataset","metadata":{"_uuid":"30e101528850aab3a8565cf99ef38ee0dd770531"}},{"cell_type":"code","source":"inputs_4 = Input(shape=(sequence_length,), dtype='int32')\nembedding_layer_4 = Embedding(num_words,\n                            embedding_dim,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=sequence_length,\n                            trainable=True)(inputs_4)\n\nreshape_4 = Reshape((sequence_length, embedding_dim, 1))(embedding_layer_4)\n\nconv_0_4 = Conv2D(num_filters, kernel_size=(3, embedding_dim), padding='valid', kernel_initializer='normal', activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_4)\nconv_1_4 = Conv2D(num_filters, kernel_size=(4, embedding_dim), padding='valid', kernel_initializer='normal', activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_4)\nconv_2_4 = Conv2D(num_filters, kernel_size=(5, embedding_dim), padding='valid', kernel_initializer='normal', activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_4)\n\nmaxpool_0_4 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0_4)\nmaxpool_1_4 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1_4)\nmaxpool_2_4 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2_4)\n\nconcatenated_tensor_4 = Concatenate(axis=1)([maxpool_0_4, maxpool_1_4, maxpool_2_4])\nflatten_4 = Flatten()(concatenated_tensor_4)\n\ndropout_4 = Dropout(0.5)(flatten_4)\n# note the different activation\noutput_4 = Dense(units=1, activation='sigmoid')(dropout_4)","metadata":{"_uuid":"1b92d6ec454dfe7e60d96367655bbb060802bae2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_4 = Model(inputs=inputs_4, outputs=output_4)\n\n# note we're using binary_crossentropy here instead of categorical\nmodel_4.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model_4.summary())","metadata":{"_uuid":"cf714659cbc8f7daa9bfd5a80a04c735e1cdffa2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\nhistory_4 = model_4.fit(sst2_X_train, sst2_y_train, epochs=30, batch_size=batch_size, verbose=1, validation_split=0.2)","metadata":{"_uuid":"e9626b4a28e974d1844d5182180c07acbbba633f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history_4.history['acc'])\nplt.plot(history_4.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\nplt.plot(history_4.history['loss'])\nplt.plot(history_4.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","metadata":{"_uuid":"79b1d3a3ebc123e078c6b1240bf96c72c050ae7e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_hat_4 = model_4.predict(sst2_X_test)\n\naccuracy_score(sst2_y_test, list(map(lambda v: v > 0.5, y_hat_4)))","metadata":{"_uuid":"e07af5424e49a3a2cf1111747a8799454b2fb20f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(sst2_y_test, list(map(lambda v: v > 0.5, y_hat_4)))","metadata":{"_uuid":"9f11553056a74d334c760c0d372e57cac5b257d9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusions\n\nHere's the final results","metadata":{"_uuid":"00cd755da0c3e70ad3bbb71dc061997052307c15"}},{"cell_type":"code","source":"print(\"CNN random       : \" + str(accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat)))))\nprint(\"CNN static       : \" + str(accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_2)))))\nprint(\"CNN trainable    : \" + str(accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_3)))))\nprint(\"Binary trainable : \" + str(accuracy_score(sst2_y_test, list(map(lambda v: v > 0.5, y_hat_4)))))","metadata":{"_uuid":"b609a6f9df9d26c1cde894f98da4625b895461ea","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see the trainable embeddings outperform the other models by quite some way. These results are pretty close to what Kim achieved\n\n### Things I learnt\n\nI learnt a lot doing this and it was well worth a weekend's hacking! I will definitely try to do more of this\n\nFirstly, it took me a long time to get anything other than random success with the w2v embeddings. My mistake was to try to clean the data in a way that didn't match the word2vec model at all. This was compounded by allocating a zero vector for words not found, the result was most of my text was labelled with the same vector (0s in 200D) and nothing was learnt\n\nSo first and second take away: look at your w2v model and **make sure you don't assign unknown words to the same vector**\n\nSecondly, as I said above I was really surprised how poorly the static w2v embedding worked. I am still not sure exactly why this would be but it might be interesting to investigate.\n\nFinally the big thing learnt here is that you should make your embeddings trainable if you can. It make a huge improvement to this model!\n\n\n# Thanks and acknowledgements\n\nObviously, thanks to Yoon Kim for writing such a [detailed and approachable paper](http://www.aclweb.org/anthology/D14-1181). I honestly didn't expect to be able to implement something to easily or achieve such good results.\n\nJoshua Kim wrote an [excellent blog post](http://www.joshuakim.io/understanding-how-convolutional-neural-network-cnn-perform-text-classification-with-word-embeddings/) explaining Yoon Kim's paper, it's well worth a read.\n\nI also found [this projct](https://github.com/bhaveshoswal/CNN-text-classification-keras/blob/master/model.py) on github which helped **a lot** when actually trying to implement this model\n\nAs mentioned, I struggled to get the word embeddings working at first but I wouldn't have even go to that point without this [blog post by Keras](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html) showing you how to use word embeddings. There is also a github repo with more detail\n\n# Lastly\n\nIf you've read this far and liked this kernel or have a question/comment/nice idea for making this better - please let me know or use the upvote thing.  It would mean a lot to me!","metadata":{"_uuid":"3aca6af717cca99dc5d3c21f5d508f87f319bdd3"}}]}